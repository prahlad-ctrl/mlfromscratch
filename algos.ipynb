{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447425ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f1c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        self.intercept = theta_best[0]\n",
    "        self.coefficients = theta_best[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(np.r_[self.intercept, self.coefficients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc1bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4107b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61a4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=3, max_iters=100):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.centroids = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        random_indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "        self.centroids = X[random_indices]\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            distances = np.array([[np.linalg.norm(x - centroid) for centroid in self.centroids] for x in X])\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "\n",
    "            new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(self.n_clusters)])\n",
    "            if np.all(self.centroids == new_centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = np.array([[np.linalg.norm(x - centroid) for centroid in self.centroids] for x in X])\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d41e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(sample, self.root) for sample in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        if len(unique_classes) == 1 or depth >= self.max_depth:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        left_indices = X[:, best_feature] < best_threshold\n",
    "        right_indices = X[:, best_feature] >= best_threshold\n",
    "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return self.Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_indices = feature_column < threshold\n",
    "        right_indices = feature_column >= threshold\n",
    "\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fc9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(sample, self.root) for sample in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if n_samples == 0 or depth >= self.max_depth:\n",
    "            leaf_value = np.mean(y)\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "        if best_feature is None:\n",
    "            leaf_value = np.mean(y)\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        left_indices = X[:, best_feature] < best_threshold\n",
    "        right_indices = X[:, best_feature] >= best_threshold\n",
    "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return self.Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7a0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_trees=10, max_depth=10, sample_size=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "        self.sample_size = self.sample_size or n_samples\n",
    "\n",
    "        for _ in range(self.n_trees):\n",
    "            indices = np.random.choice(n_samples, self.sample_size, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
    "        y_pred = [self._most_common_label(tree_pred) for tree_pred in tree_preds]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a479b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_trees=10, max_depth=10, sample_size=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "        self.sample_size = self.sample_size or n_samples\n",
    "\n",
    "        for _ in range(self.n_trees):\n",
    "            indices = np.random.choice(n_samples, self.sample_size, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
    "        y_pred = [np.mean(tree_pred) for tree_pred in tree_preds]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50af4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvmClassifier:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[idx]))\n",
    "                    self.bias -= self.learning_rate * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) - self.bias\n",
    "        return np.sign(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03d8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.mean = {}\n",
    "        self.variance = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "\n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            self.class_priors[cls] = X_c.shape[0] / n_samples\n",
    "            self.mean[cls] = X_c.mean(axis=0)\n",
    "            self.variance[cls] = X_c.var(axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict_sample(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict_sample(self, x):\n",
    "        posteriors = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])\n",
    "            likelihood = np.sum(np.log(self._gaussian_probability(cls, x)))\n",
    "            posterior = prior + likelihood\n",
    "            posteriors[cls] = posterior\n",
    "\n",
    "        return max(posteriors, key=posteriors.get)\n",
    "\n",
    "    def _gaussian_probability(self, cls, x):\n",
    "        mean = self.mean[cls]\n",
    "        variance = self.variance[cls]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * variance))\n",
    "        denominator = np.sqrt(2 * np.pi * variance)\n",
    "        return numerator / denominator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
