{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924f0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0b0dc",
   "metadata": {},
   "source": [
    "Accuracy - used to rpedict the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f102769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = y_true.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa1a5d",
   "metadata": {},
   "source": [
    "Mean squared error - used to measure the average of the squared difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e57ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e9da8",
   "metadata": {},
   "source": [
    "Mean absolute error - used to measure the average of absolute difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f49972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa704d90",
   "metadata": {},
   "source": [
    "Root mean squared error - applying root to the average of squared difference between predicted and actual values (lower means better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c483ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd2036",
   "metadata": {},
   "source": [
    "R2 score - shows the fit of regression model lying between 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3f65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f05b3",
   "metadata": {},
   "source": [
    "Precision score - measures the accuracy of positive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656c4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred):\n",
    "    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    predicted_positives = np.sum(y_pred == 1)\n",
    "    return true_positives / predicted_positives if predicted_positives > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023172d",
   "metadata": {},
   "source": [
    "Recall score - measures the actual positive that model correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15bdf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(y_true, y_pred):\n",
    "    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    actual_positives = np.sum(y_true == 1)\n",
    "    return true_positives / actual_positives if actual_positives > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ec41c",
   "metadata": {},
   "source": [
    "F1 score - mean of precision and recall score (both high for good score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36bb35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea9acc",
   "metadata": {},
   "source": [
    "Confusion matricx - table used to emasure how well a classification model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fb5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "    for i, true_class in enumerate(classes):\n",
    "        for j, pred_class in enumerate(classes):\n",
    "            matrix[i, j] = np.sum((y_true == true_class) & (y_pred == pred_class))\n",
    "    return matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
